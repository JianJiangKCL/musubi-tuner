# LoRA-MoE Stage A Configuration: Base LoRA Training
# General adaptation on surgical video data

[general]
task = "t2v-A14B"  # WAN 2.2 Text-to-Video
training_stage = "stage_a"
output_dir = "./outputs/lora_moe_stage_a"
logging_dir = "./logs/lora_moe_stage_a"

[model]
# WAN model paths
dit = "./models/wan2.2/wan_t2v_A14B.safetensors"
dit_high_noise = "./models/wan2.2/wan_t2v_A14B_high_noise.safetensors"
vae = "./models/wan2.2/vae.safetensors"
t5 = "./models/t5-v1_1-xxl"
clip = "./models/clip-vit-large-patch14"

[lora_moe]
# Base LoRA configuration
lora_dim = 4
lora_alpha = 1.0
num_experts = 4
expert_names = ["Scissors", "Hook/Electrocautery", "Suction", "Other"]
use_base_lora = true

# Dropouts (low for 160 videos)
lora_dropout = 0.0
lora_rank_dropout = 0.0
lora_module_dropout = 0.0

# Projection-specific ranks
rank_q = 8   # Query most impactful
rank_k = 4
rank_v = 4
rank_o = 4
rank_ffn = 4

# Target blocks (last 8 of 40 blocks)
target_blocks = "last_8"  # Indices 32-39

[router]
# Router not used in Stage A (experts frozen)
routing_mode = "rule_based"
router_top_k = 2
router_temperature = 0.7
router_ema_beta = 0.9

[training]
# Training hyperparameters
batch_size = 1  # Adjust based on VRAM
gradient_accumulation_steps = 8
num_train_epochs = 10
learning_rate = 1e-4
lr_scheduler = "constant_with_warmup"
lr_warmup_steps = 100

# Mixed precision
mixed_precision = "bf16"

# Gradient settings
max_grad_norm = 1.0

# Checkpointing
save_steps = 500
save_total_limit = 3

# Logging
logging_steps = 10

[data]
# Dataset configuration
train_data_dir = "./data/surgical_videos"
instrument_data_path = "./Lap/preprocessing/filtered_clips_processed.jsonl"

# Video parameters (from WAN2.2 config)
frame_num = 81
resolution = "720p"  # 1280x720
frame_sample_method = "uniform"

# Preprocessing
random_flip = false  # No flip for surgical videos
random_crop = false

[loss]
# Loss weights for Stage A
weight_base_diffusion = 1.0
weight_roi_recon = 3.0      # Emphasize ROI
weight_identity = 0.0       # No identity loss in Stage A
weight_temporal = 0.0       # No temporal loss in Stage A
weight_routing_entropy = 0.0      # No routing in Stage A
weight_routing_load_balance = 0.0 # No routing in Stage A

# Loss options
use_roi_loss = true
use_identity_loss = false
use_temporal_loss = false

[optimizer]
optimizer_type = "AdamW"
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_epsilon = 1e-8
weight_decay = 0.01

[misc]
# Seed for reproducibility
seed = 42

# Number of workers for data loading
num_workers = 4

# Enable gradient checkpointing to save memory
gradient_checkpointing = true

# Offload inactive DiT to CPU (for WAN2.2 dual-model)
offload_inactive_dit = true
