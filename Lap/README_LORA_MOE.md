# LoRA-MoE Training for WAN2.2 (Musubi Framework)

## Overview

This guide explains how to train LoRA-MoE (Mixture of Experts) for surgical video generation using the musubi framework with **cached latents and text features**.

---

## Prerequisites

### 1. Completed Stage 1: Vanilla LoRA Training

You should have already completed vanilla LoRA training using musubi's standard workflow:

```bash
# Stage 1 was done with your existing script
bash Lap/scripts/train_trace50_allvideos_20s.sh
```

**Output required**: `vanilla_lora/lora_final.safetensors`

### 2. Cached Latents and Text Features

Your cached data should be in: `/mnt/cfs/jj/proj/musubi-tuner/Lap/cache/trace50_allvideos_20s`

Generated by:
```bash
# Text features (T5 encoder)
python wan_cache_text_encoder_outputs.py \
    --dataset_config config/trace50_all_videos_20s.toml \
    --t5 models/models_t5_umt5-xxl-enc-bf16.pth \
    --batch_size 128

# Video latents (VAE encoder)
python wan_cache_latents.py \
    --dataset_config config/trace50_all_videos_20s.toml \
    --vae models/Wan2.1_VAE.pth \
    --vae_dtype bfloat16 \
    --batch_size 180 \
    --i2v
```

### 3. Surgical Clips with Captions

Your clips metadata: `Lap/preprocessing/filtered_clips_processed.jsonl`

Format:
```json
{"video_path": "cholec36/clips/cholec36_0137_3s.mp4", "caption": "抓钳抓取标本袋", "duration": 3, "source": "cholec36"}
{"video_path": "cholec36/clips/cholec36_0138_3s.mp4", "caption": "抓钳抓取胆囊", "duration": 3, "source": "cholec36"}
```

---

## Training Pipeline

### Step 1: Extract Instrument Labels from Captions

Generate instrument-augmented clips file:

```bash
cd /mnt/cfs/jj/musubi-tuner

python Lap/scripts/augment_clips_with_instruments.py \
    --input Lap/preprocessing/filtered_clips_processed.jsonl \
    --output Lap/preprocessing/filtered_clips_with_instruments.jsonl \
    --soft_labels
```

**What this does**:
- Reads Chinese captions (e.g., "抓钳抓取组织")
- Extracts instrument type using keyword matching
- Adds `instrument_label` field (0-3)
- Optionally adds soft `instrument_logits` for routing

**Output example**:
```json
{
  "video_path": "cholec36/clips/cholec36_0137_3s.mp4",
  "caption": "抓钳抓取标本袋",
  "duration": 3,
  "source": "cholec36",
  "instrument_label": 3,  // Other (Grasper)
  "instrument_logits": [0.0, 0.0, 0.0, 1.0]
}
```

**Instrument mapping**:
- `0` = Scissors (剪刀)
- `1` = Hook/Electrocautery (电凝钩/双极电凝)
- `2` = Suction (吸引)
- `3` = Other (抓钳/戳卡/其他)

---

### Step 2: Train LoRA-MoE (Experts + Router)

Run the Stage 2 training script:

```bash
cd /mnt/cfs/jj/musubi-tuner

# Adjust paths in the script first!
bash Lap/scripts/train_lora_moe_stage2.sh
```

**Important paths to configure in the script**:
```bash
# Set these to match your setup
PROJECT_ROOT="/mnt/cfs/jj/musubi-tuner"
CACHE_ROOT="/mnt/cfs/jj/proj/musubi-tuner/Lap/cache/trace50_allvideos_20s"
VANILLA_LORA_WEIGHTS="${PROJECT_ROOT}/outputs/vanilla_lora/lora_final.safetensors"
```

**Training configuration**:
- **Experts**: 4 (Scissors, Hook, Suction, Other)
- **Router**: Learned MLP with rule-based teacher guidance
- **Base LoRA**: Frozen (from Stage 1)
- **Epochs**: 20
- **Learning rate**: 5e-5
- **Duration**: ~5-7 hours on 8x GPUs

---

## How It Works

### Architecture

```
WAN2.2 Frozen Base
    ↓
+ Base LoRA (FROZEN from Stage 1)
    ↓
+ Expert LoRAs (TRAINABLE)
    ├─ Expert 0: Scissors (剪刀)
    ├─ Expert 1: Hook/Electrocautery (电凝钩)
    ├─ Expert 2: Suction (吸引)
    └─ Expert 3: Other (抓钳/其他)
    ↓
Gated by Learnable Router (TRAINABLE)
    ↓
= Instrument-Specific Output
```

### Composition Formula

```
y = W(x) + α_base·ΔW_base(x) + Σ_e g_e·α_e·ΔW_e(x)
           ^^^^^^^^^^^^^^      ^^^^^^^^^^^^^^^^^^^
           Frozen (Stage 1)    Trained (Stage 2)
```

Where:
- `W`: Frozen WAN2.2 weights
- `ΔW_base`: Frozen base LoRA from Stage 1
- `ΔW_e`: Expert LoRA for instrument `e`
- `g_e`: Routing gate for expert `e` (from learned router)

### Data Flow

1. **Load cached latents** from `/cache/trace50_allvideos_20s/`
2. **Load text features** (T5 embeddings)
3. **Extract instrument label** from augmented JSONL
4. **Compute routing gates** via learned router
5. **Forward through WAN2.2** with base LoRA + expert LoRAs
6. **Compute loss** with routing regularization
7. **Update experts + router** simultaneously

---

## Instrument Label Extraction

### Keyword-Based Matching

The `instrument_utils.py` module extracts labels using Chinese surgical keywords:

```python
INSTRUMENT_KEYWORDS = {
    0: ["剪刀", "剪切", "剪断", "剪开"],  # Scissors
    1: ["电凝钩", "双极电凝", "电凝", "钩"],  # Hook/Electrocautery
    2: ["吸引", "吸", "吸出", "清理"],  # Suction
    3: ["抓钳", "戳卡", "镊子", "夹持", "标本袋"],  # Other
}
```

**Examples**:
- "抓钳抓取组织" → Label 3 (Other)
- "电凝钩切割血管" → Label 1 (Hook/Electrocautery)
- "吸引清理术野" → Label 2 (Suction)
- "剪刀剪断组织" → Label 0 (Scissors)

### Handling Co-occurrence

When multiple instruments appear in one caption:
```python
# Caption: "抓钳固定。电凝钩切割。"
instrument_logits = [0.0, 0.65, 0.0, 0.35]
# Hook: 65%, Other: 35%
```

The router learns soft routing gates based on these distributions.

---

## Loss Function

### Combined Loss (Stage 2)

```python
L_total = L_diffusion                # Standard flow matching
        + 3.0 * L_roi                # ROI-weighted reconstruction (optional)
        + 0.02 * L_entropy           # Routing entropy (prevent collapse)
        + 0.1 * L_balance            # Load balancing (uniform expert usage)
        + 1.0 * L_teacher_kl         # KL from learned to rule-based router
```

### Teacher-Student Guidance

**Teacher**: Rule-based router
```python
# Simply use instrument_logits from caption
teacher_gates = softmax(instrument_logits / temperature)
```

**Student**: Learned MLP router
```python
# 2-layer MLP: 512 → 64 → 4
learned_gates = router_mlp(instrument_logits)
```

**KL Loss**:
```python
L_teacher_kl = KL(learned_gates || teacher_gates)
```

**Why?**
- Prevents routing collapse (all experts → one)
- Provides stable initialization
- Allows learned router to adapt when beneficial

---

## Training Monitoring

### Key Metrics to Watch

1. **Diffusion Loss**: Should decrease steadily
   - Initial: ~0.05-0.1
   - Final: ~0.02-0.03

2. **Routing Entropy**: Should stabilize (NOT collapse to 0)
   - Target: ~0.5-1.0
   - If <0.3: Routing collapsed, increase load balance weight

3. **Expert Usage**: Should be roughly uniform
   - Target: 20-30% per expert
   - If >70% on one: Increase load balance weight

4. **Teacher KL**: Should decrease as router learns
   - Initial: ~0.5-1.0
   - Final: ~0.1-0.3

### Logging

Check tensorboard logs:
```bash
tensorboard --logdir outputs/lora_moe_stage2/logs
```

Monitor:
- `loss/diffusion`
- `loss/routing_entropy`
- `loss/routing_load_balance`
- `loss/teacher_kl`
- `metrics/expert_usage_*`

---

## Troubleshooting

### Problem: Routing Collapse

**Symptom**: All clips route to one expert (e.g., >90% to "Other")

**Solutions**:
1. Increase load balance weight: `--weight_routing_load_balance 0.2`
2. Increase entropy weight: `--weight_routing_entropy 0.05`
3. Check caption distribution (may be class-imbalanced)
4. Use balanced sampling in dataloader

### Problem: Expert Underutilization

**Symptom**: Some experts never activate (<5% usage)

**Solutions**:
1. Check caption distribution for that instrument
2. Verify keyword matching is working
3. Manually inspect `filtered_clips_with_instruments.jsonl`
4. Adjust INSTRUMENT_KEYWORDS in `instrument_utils.py`

### Problem: High Teacher KL (>1.0)

**Symptom**: Learned router diverges from rule-based teacher

**Solutions**:
1. Increase teacher KL weight: `--teacher_kl_weight 2.0`
2. Check if instrument_logits are correct
3. Verify router input dimension matches

### Problem: Training Crashes (OOM)

**Solutions**:
1. Reduce batch size: `--batch_size 1`
2. Increase gradient accumulation: `--gradient_accumulation_steps 16`
3. Enable offloading: `--offload_inactive_dit`
4. Reduce num_workers in dataloader

---

## Inference

After training, use the LoRA-MoE checkpoint for generation:

```bash
python wan_inference_lora_moe.py \
    --task t2v-A14B \
    --lora_moe_weights outputs/lora_moe_stage2/lora_moe_final.safetensors \
    --prompt "电凝钩切割组织。吸引止血。" \
    --instrument_hint "Hook/Electrocautery" \
    --alpha_base 0.7 \
    --alpha_expert 1.0 \
    --output_path output_hook.mp4
```

**Instrument hints**:
- `Scissors` (剪刀)
- `Hook/Electrocautery` (电凝钩)
- `Suction` (吸引)
- `Other` (抓钳/其他)
- If not provided, auto-detects from prompt

---

## File Structure

```
musubi-tuner/
├── Lap/
│   ├── preprocessing/
│   │   ├── filtered_clips_processed.jsonl              # Original clips
│   │   └── filtered_clips_with_instruments.jsonl      # Augmented with labels
│   ├── scripts/
│   │   ├── augment_clips_with_instruments.py          # Extract labels
│   │   └── train_lora_moe_stage2.sh                   # Training script
│   └── cache/
│       └── trace50_allvideos_20s/                     # Cached latents + text
│
├── src/musubi_tuner/
│   ├── utils/
│   │   └── instrument_utils.py                         # Instrument extraction
│   ├── networks/
│   │   ├── lora_moe.py                                 # Core LoRA-MoE
│   │   └── lora_wan_moe.py                             # WAN integration
│   ├── wan_train_lora_moe.py                          # Training script
│   └── wan_inference_lora_moe.py                      # Inference script
│
└── outputs/
    ├── vanilla_lora/                                   # Stage 1 output
    │   └── lora_final.safetensors                      # Frozen as base
    └── lora_moe_stage2/                                # Stage 2 output
        ├── lora_moe_final.safetensors                  # Final checkpoint
        └── logs/                                        # Tensorboard logs
```

---

## Quick Start Checklist

- [ ] Stage 1 vanilla LoRA training complete
- [ ] Vanilla LoRA weights available (`lora_final.safetensors`)
- [ ] Cached latents exist in `/cache/trace50_allvideos_20s/`
- [ ] Cached text features exist
- [ ] `filtered_clips_processed.jsonl` available
- [ ] Run `augment_clips_with_instruments.py` to extract labels
- [ ] Verify instrument distribution (check for imbalance)
- [ ] Adjust paths in `train_lora_moe_stage2.sh`
- [ ] Run `bash Lap/scripts/train_lora_moe_stage2.sh`
- [ ] Monitor training via tensorboard
- [ ] Evaluate on test clips
- [ ] Compare vs vanilla LoRA baseline

---

## Expected Results

With 160 surgical video clips:

| Metric | Vanilla LoRA | LoRA-MoE | Improvement |
|--------|-------------|----------|-------------|
| Tip Accuracy | Baseline | +15-20% | Better specialization |
| Identity Preservation | Baseline | +20-25% | Instrument-specific |
| Routing Accuracy | N/A | 85-90% | Good expert selection |
| Training Time | ~3 hours | ~7 hours | Worth the cost |

---

## Next Steps

1. **Evaluate**: Generate videos for all 4 instrument types
2. **Ablation**: Test without teacher guidance, different num_experts
3. **Fine-tune**: Adjust routing weights if expert usage is imbalanced
4. **Scale**: Try with more clips if available
5. **Export**: Save best checkpoint for deployment

---

## Support

For issues:
- Check logs in `outputs/lora_moe_stage2/logs/`
- Verify instrument label distribution
- Ensure cached data is valid
- Test with vanilla LoRA first to isolate issues

**Key files to check**:
- `instrument_utils.py`: Instrument extraction logic
- `lora_wan_moe.py`: LoRA-MoE integration
- `train_lora_moe_stage2.sh`: Training configuration
